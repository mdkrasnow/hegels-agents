#!/usr/bin/env python3
"""
Phase 0.5 Integration Testing Script

This script performs comprehensive integration testing of all Phase 0.5 components
to validate that they work together seamlessly. Tests the complete pipeline:
Agent ‚Üí Corpus ‚Üí Dialectical Testing end-to-end workflow.

This is the final validation before Phase 0.5 completion.

Usage:
    python scripts/integration_test_phase_0_5.py [--mock] [--verbose] [--output DIR]
"""

import sys
import argparse
import json
import time
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import asdict

# Add src to path for imports
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root / "src"))
sys.path.append(str(project_root))

# Core imports
from config.settings import load_config
from agents.worker import BasicWorkerAgent
from agents.reviewer import BasicReviewerAgent
from agents.utils import AgentResponse
from corpus.file_retriever import FileCorpusRetriever
from debate.dialectical_tester import DialecticalTester
from debate.session import DebateSession
from test_questions.dialectical_test_questions import get_question_set


class Phase05IntegrationTester:
    """
    Comprehensive integration tester for all Phase 0.5 components.
    """
    
    def __init__(self, config_path: str = None, mock_mode: bool = False):
        """
        Initialize integration tester.
        
        Args:
            config_path: Path to configuration file
            mock_mode: Whether to run in mock mode (no API calls)
        """
        self.mock_mode = mock_mode
        
        # Handle configuration loading with graceful fallback for mock mode
        try:
            self.config = load_config(config_path)
        except Exception as e:
            if self.mock_mode:
                print(f"‚ö†Ô∏è  Configuration loading failed, but continuing in mock mode: {e}")
                # Create minimal mock config
                self.config = {"mock_mode": True}
            else:
                raise
        
        self.test_results = {}
        self.start_time = datetime.now()
        
        # Initialize components
        self.corpus_retriever = None
        self.worker_1 = None
        self.worker_2 = None
        self.reviewer = None
        self.dialectical_tester = None
        
    def setup_test_environment(self) -> Dict[str, Any]:
        """
        Set up and validate the complete test environment.
        
        Returns:
            Environment status dictionary
        """
        print("üîß Setting up Phase 0.5 integration test environment...")
        
        env_status = {
            "timestamp": datetime.now().isoformat(),
            "mock_mode": self.mock_mode,
            "components": {},
            "errors": []
        }
        
        try:
            # 1. Configuration System Test
            print("  ‚úì Testing configuration system...")
            if not self.config:
                raise Exception("Configuration failed to load")
            env_status["components"]["configuration"] = "‚úì PASS"
            
            # 2. Corpus System Test
            print("  ‚úì Testing corpus retrieval system...")
            self.corpus_retriever = FileCorpusRetriever(
                corpus_dir=str(project_root / "corpus_data")
            )
            
            # Test corpus availability
            corpus_files = list((project_root / "corpus_data").glob("*.txt"))
            if len(corpus_files) < 5:
                env_status["errors"].append(f"Insufficient corpus files: {len(corpus_files)}")
            
            # Load and index the corpus for testing
            print("    - Loading corpus...")
            load_result = self.corpus_retriever.load_corpus()
            print("    - Building search index...")
            index_result = self.corpus_retriever.build_search_index()
            
            # Test retrieval functionality
            test_retrieval = self.corpus_retriever.retrieve_for_question(
                "What is quantum mechanics?", max_results=2
            )
            if not test_retrieval:
                env_status["errors"].append("Corpus retrieval returned empty results")
            
            env_status["components"]["corpus"] = "‚úì PASS"
            print(f"    - Found {len(corpus_files)} corpus files")
            print(f"    - Test retrieval: {len(test_retrieval.split()[:10])} words...")
            
            # 3. Agent System Test
            print("  ‚úì Testing agent system...")
            
            if self.mock_mode:
                # Use mock agents for testing
                from unittest.mock import MagicMock
                
                self.worker_1 = MagicMock()
                self.worker_1.respond.return_value = AgentResponse(
                    content="Mock worker 1 response",
                    reasoning="Mock reasoning",
                    confidence=0.8,
                    sources=["mock_source"],
                    metadata={"agent": "mock_worker_1"}
                )
                
                self.worker_2 = MagicMock()
                self.worker_2.respond.return_value = AgentResponse(
                    content="Mock worker 2 response with different perspective",
                    reasoning="Alternative mock reasoning",
                    confidence=0.7,
                    sources=["mock_source"],
                    metadata={"agent": "mock_worker_2"}
                )
                
                self.reviewer = MagicMock()
                self.reviewer.synthesize_responses.return_value = AgentResponse(
                    content="Mock synthesis combining both perspectives",
                    reasoning="Synthesis reasoning",
                    confidence=0.9,
                    sources=["synthesis"],
                    metadata={"agent": "mock_reviewer"}
                )
                
                # Mock the Gemini call for quality evaluation
                self.reviewer._make_gemini_call.return_value = "7"
                
                print("    - Using mock agents for testing")
            else:
                # Use real agents
                self.worker_1 = BasicWorkerAgent(
                    name="Integration_Test_Worker_1",
                    config=self.config
                )
                
                self.worker_2 = BasicWorkerAgent(
                    name="Integration_Test_Worker_2", 
                    config=self.config
                )
                
                self.reviewer = BasicReviewerAgent(
                    name="Integration_Test_Reviewer",
                    config=self.config
                )
                
                print("    - Real agents initialized for API testing")
            
            env_status["components"]["agents"] = "‚úì PASS"
            
            # 4. Dialectical Testing System
            print("  ‚úì Testing dialectical testing framework...")
            
            # For mock mode, manually set up the tester with our mock agents
            if self.mock_mode:
                # Use a mock DialecticalTester that we can inject our mock agents into
                from unittest.mock import MagicMock
                self.dialectical_tester = MagicMock()
                self.dialectical_tester.worker_1 = self.worker_1
                self.dialectical_tester.worker_2 = self.worker_2
                self.dialectical_tester.reviewer = self.reviewer
                self.dialectical_tester.corpus_retriever = self.corpus_retriever
                
                # Mock the test methods
                def mock_single_test(question):
                    return self.worker_1.respond(question), 1.0
                
                def mock_dialectical_test(question):
                    return self.reviewer.synthesize_responses(question, []), MagicMock(), 2.0
                
                def mock_comparison_test(question):
                    from debate.dialectical_tester import DialecticalTestResult
                    from datetime import datetime
                    
                    single_response = self.worker_1.respond(question)
                    synthesis_response = self.reviewer.synthesize_responses(question, [])
                    
                    return DialecticalTestResult(
                        question=question,
                        single_agent_response=single_response,
                        dialectical_synthesis=synthesis_response,
                        debate_session=MagicMock(),
                        single_agent_quality_score=7.0,
                        dialectical_quality_score=8.0,
                        improvement_score=14.3,
                        single_agent_time=1.0,
                        dialectical_time=2.0,
                        conflict_identified=True,
                        synthesis_effectiveness=0.8,
                        timestamp=datetime.now(),
                        test_metadata={}
                    )
                
                self.dialectical_tester.run_single_agent_test = mock_single_test
                self.dialectical_tester.run_dialectical_test = mock_dialectical_test
                self.dialectical_tester.run_comparison_test = mock_comparison_test
            else:
                # Use real DialecticalTester
                self.dialectical_tester = DialecticalTester(
                    corpus_dir=str(project_root / "corpus_data")
                )
            
            env_status["components"]["dialectical_tester"] = "‚úì PASS"
            
            print("‚úÖ Environment setup complete")
            return env_status
            
        except Exception as e:
            error_msg = f"Environment setup failed: {str(e)}"
            env_status["errors"].append(error_msg)
            env_status["status"] = "FAILED"
            print(f"‚ùå {error_msg}")
            return env_status
    
    def test_component_integration(self) -> Dict[str, Any]:
        """
        Test integration between all components.
        
        Returns:
            Integration test results
        """
        print("\nüîó Testing component integration...")
        
        integration_results = {
            "timestamp": datetime.now().isoformat(),
            "tests": {},
            "errors": [],
            "overall_status": "UNKNOWN"
        }
        
        try:
            # Test 1: Agent ‚Üí Corpus Integration
            print("  ‚úì Testing Agent ‚Üí Corpus integration...")
            
            test_question = "What are the key principles of quantum mechanics?"
            context = self.corpus_retriever.retrieve_for_question(test_question, max_results=2)
            
            if self.mock_mode:
                response = self.worker_1.respond(test_question, external_context=context)
            else:
                response = self.worker_1.respond(test_question, external_context=context)
                
            if not response or not response.content:
                raise Exception("Agent failed to respond with corpus context")
                
            integration_results["tests"]["agent_corpus"] = {
                "status": "‚úì PASS",
                "response_length": len(response.content),
                "context_provided": len(context) if context else 0
            }
            
            # Test 2: Dialectical Workflow Integration  
            print("  ‚úì Testing complete dialectical workflow...")
            
            single_response, single_time = self.dialectical_tester.run_single_agent_test(test_question)
            dialectical_response, debate_session, dialectical_time = self.dialectical_tester.run_dialectical_test(test_question)
            
            if not single_response or not dialectical_response:
                raise Exception("Dialectical workflow failed to produce responses")
                
            integration_results["tests"]["dialectical_workflow"] = {
                "status": "‚úì PASS", 
                "single_response_time": single_time,
                "dialectical_response_time": dialectical_time,
                "debate_turns": len(debate_session.turns),
                "conflicts_detected": debate_session.conflicts_detected,
                "synthesis_effectiveness": debate_session.synthesis_effectiveness_score
            }
            
            # Test 3: Error Handling Integration
            print("  ‚úì Testing error handling integration...")
            
            try:
                # Test with invalid question
                invalid_response, _ = self.dialectical_tester.run_single_agent_test("")
                
                # Should handle gracefully
                integration_results["tests"]["error_handling"] = {
                    "status": "‚úì PASS",
                    "handles_empty_questions": True
                }
            except Exception as e:
                integration_results["tests"]["error_handling"] = {
                    "status": "‚úì PASS (Expected failure)",
                    "error_caught": str(e)
                }
            
            # Test 4: Performance Integration
            print("  ‚úì Testing performance integration...")
            
            perf_start = time.time()
            
            # Run mini performance test
            test_questions = get_question_set()[:2]  # Just 2 questions for integration test
            
            perf_results = []
            for question in test_questions:
                start = time.time()
                single_resp, single_t = self.dialectical_tester.run_single_agent_test(question)
                end = time.time()
                
                perf_results.append({
                    "question_length": len(question),
                    "response_length": len(single_resp.content),
                    "time_taken": end - start
                })
            
            avg_time = sum(r["time_taken"] for r in perf_results) / len(perf_results)
            
            integration_results["tests"]["performance"] = {
                "status": "‚úì PASS",
                "questions_tested": len(test_questions),
                "average_response_time": avg_time,
                "total_test_time": time.time() - perf_start
            }
            
            integration_results["overall_status"] = "‚úÖ ALL TESTS PASSED"
            print("‚úÖ Component integration tests complete")
            
        except Exception as e:
            error_msg = f"Integration test failed: {str(e)}"
            integration_results["errors"].append(error_msg)
            integration_results["overall_status"] = "‚ùå TESTS FAILED"
            print(f"‚ùå {error_msg}")
            if not self.mock_mode:
                print("Traceback:", traceback.format_exc())
        
        return integration_results
    
    def test_end_to_end_workflow(self) -> Dict[str, Any]:
        """
        Test complete end-to-end workflow with real examples.
        
        Returns:
            End-to-end test results
        """
        print("\nüîÑ Testing end-to-end workflow...")
        
        e2e_results = {
            "timestamp": datetime.now().isoformat(), 
            "workflow_tests": [],
            "summary": {},
            "errors": []
        }
        
        try:
            # Get test questions
            test_questions = get_question_set()[:3]  # 3 questions for thorough but fast test
            
            print(f"  Running complete workflow on {len(test_questions)} questions...")
            
            total_improvements = []
            successful_tests = 0
            
            for i, question in enumerate(test_questions):
                print(f"    Test {i+1}: {question[:50]}...")
                
                try:
                    # Run complete comparison
                    test_result = self.dialectical_tester.run_comparison_test(question)
                    
                    workflow_result = {
                        "question_id": i + 1,
                        "question": question,
                        "single_agent_score": test_result.single_agent_quality_score,
                        "dialectical_score": test_result.dialectical_quality_score,
                        "improvement_percentage": test_result.improvement_score,
                        "single_time": test_result.single_agent_time,
                        "dialectical_time": test_result.dialectical_time,
                        "conflicts_detected": test_result.conflict_identified,
                        "synthesis_effectiveness": test_result.synthesis_effectiveness,
                        "status": "‚úì PASS"
                    }
                    
                    e2e_results["workflow_tests"].append(workflow_result)
                    total_improvements.append(test_result.improvement_score)
                    successful_tests += 1
                    
                except Exception as e:
                    error_result = {
                        "question_id": i + 1,
                        "question": question,
                        "error": str(e),
                        "status": "‚ùå FAIL"
                    }
                    e2e_results["workflow_tests"].append(error_result)
                    e2e_results["errors"].append(f"Question {i+1}: {str(e)}")
            
            # Calculate summary statistics
            if total_improvements:
                e2e_results["summary"] = {
                    "successful_tests": successful_tests,
                    "total_tests": len(test_questions),
                    "success_rate": successful_tests / len(test_questions) * 100,
                    "average_improvement": sum(total_improvements) / len(total_improvements),
                    "improvement_range": [min(total_improvements), max(total_improvements)],
                    "tests_showing_improvement": sum(1 for imp in total_improvements if imp > 0),
                    "overall_status": "‚úÖ SUCCESS" if successful_tests > len(test_questions) * 0.7 else "‚ö†Ô∏è PARTIAL"
                }
            else:
                e2e_results["summary"]["overall_status"] = "‚ùå FAILED"
            
            print("‚úÖ End-to-end workflow testing complete")
            
        except Exception as e:
            error_msg = f"End-to-end test failed: {str(e)}"
            e2e_results["errors"].append(error_msg)
            e2e_results["summary"]["overall_status"] = "‚ùå FAILED"
            print(f"‚ùå {error_msg}")
        
        return e2e_results
    
    def run_comprehensive_integration_test(self) -> Dict[str, Any]:
        """
        Run comprehensive integration test of entire Phase 0.5 system.
        
        Returns:
            Complete test results
        """
        print("üöÄ Starting Phase 0.5 Comprehensive Integration Test")
        print(f"Mode: {'Mock Testing' if self.mock_mode else 'Live API Testing'}")
        print("=" * 60)
        
        comprehensive_results = {
            "test_metadata": {
                "timestamp": self.start_time.isoformat(),
                "mode": "mock" if self.mock_mode else "live",
                "version": "0.5.4"
            },
            "environment_setup": {},
            "component_integration": {},
            "end_to_end_workflow": {},
            "performance_benchmarks": {},
            "final_assessment": {}
        }
        
        try:
            # 1. Environment Setup
            comprehensive_results["environment_setup"] = self.setup_test_environment()
            
            if comprehensive_results["environment_setup"].get("status") == "FAILED":
                comprehensive_results["final_assessment"]["status"] = "‚ùå ENVIRONMENT_FAILED"
                return comprehensive_results
            
            # 2. Component Integration Testing
            comprehensive_results["component_integration"] = self.test_component_integration()
            
            # 3. End-to-End Workflow Testing
            comprehensive_results["end_to_end_workflow"] = self.test_end_to_end_workflow()
            
            # 4. Performance Benchmarking
            comprehensive_results["performance_benchmarks"] = self.run_performance_benchmarks()
            
            # 5. Final Assessment
            comprehensive_results["final_assessment"] = self.generate_final_assessment(comprehensive_results)
            
        except Exception as e:
            comprehensive_results["final_assessment"] = {
                "status": "‚ùå CRITICAL_FAILURE",
                "error": str(e),
                "traceback": traceback.format_exc() if not self.mock_mode else "Mock mode - no traceback"
            }
        
        return comprehensive_results
    
    def run_performance_benchmarks(self) -> Dict[str, Any]:
        """
        Run performance benchmarking of complete system.
        
        Returns:
            Performance benchmark results
        """
        print("\n‚ö° Running performance benchmarks...")
        
        perf_results = {
            "timestamp": datetime.now().isoformat(),
            "benchmarks": {},
            "summary": {}
        }
        
        try:
            # Benchmark 1: Single Agent Performance
            print("  ‚úì Benchmarking single agent performance...")
            
            single_times = []
            for _ in range(3):  # 3 runs for average
                start = time.time()
                response, _ = self.dialectical_tester.run_single_agent_test("What is artificial intelligence?")
                end = time.time()
                single_times.append(end - start)
            
            perf_results["benchmarks"]["single_agent"] = {
                "average_time": sum(single_times) / len(single_times),
                "min_time": min(single_times),
                "max_time": max(single_times),
                "runs": len(single_times)
            }
            
            # Benchmark 2: Dialectical Process Performance
            print("  ‚úì Benchmarking dialectical process...")
            
            dialectical_times = []
            for _ in range(3):
                start = time.time()
                response, session, _ = self.dialectical_tester.run_dialectical_test("What is machine learning?")
                end = time.time()
                dialectical_times.append(end - start)
            
            perf_results["benchmarks"]["dialectical_process"] = {
                "average_time": sum(dialectical_times) / len(dialectical_times),
                "min_time": min(dialectical_times),
                "max_time": max(dialectical_times),
                "runs": len(dialectical_times)
            }
            
            # Performance Summary
            single_avg = perf_results["benchmarks"]["single_agent"]["average_time"]
            dialectical_avg = perf_results["benchmarks"]["dialectical_process"]["average_time"]
            
            perf_results["summary"] = {
                "single_agent_avg_time": single_avg,
                "dialectical_avg_time": dialectical_avg,
                "time_overhead": dialectical_avg - single_avg,
                "overhead_percentage": ((dialectical_avg - single_avg) / single_avg) * 100,
                "performance_acceptable": (dialectical_avg - single_avg) < 60.0,  # Less than 60s overhead
                "status": "‚úÖ ACCEPTABLE" if (dialectical_avg - single_avg) < 60.0 else "‚ö†Ô∏è HIGH_OVERHEAD"
            }
            
            print(f"    - Single agent: {single_avg:.2f}s avg")
            print(f"    - Dialectical: {dialectical_avg:.2f}s avg")
            print(f"    - Overhead: {dialectical_avg - single_avg:.2f}s ({perf_results['summary']['overhead_percentage']:.1f}%)")
            
        except Exception as e:
            perf_results["error"] = str(e)
            perf_results["status"] = "‚ùå FAILED"
            print(f"‚ùå Performance benchmarking failed: {str(e)}")
        
        return perf_results
    
    def generate_final_assessment(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate final assessment of Phase 0.5 integration.
        
        Args:
            test_results: Complete test results
            
        Returns:
            Final assessment
        """
        print("\nüìä Generating final assessment...")
        
        assessment = {
            "timestamp": datetime.now().isoformat(),
            "overall_status": "UNKNOWN",
            "component_status": {},
            "integration_quality": {},
            "readiness_assessment": {},
            "recommendations": []
        }
        
        try:
            # Assess each component
            env_status = test_results.get("environment_setup", {})
            integration_status = test_results.get("component_integration", {})
            e2e_status = test_results.get("end_to_end_workflow", {})
            perf_status = test_results.get("performance_benchmarks", {})
            
            # Component Status Assessment
            assessment["component_status"] = {
                "environment": "‚úÖ READY" if not env_status.get("errors") else "‚ùå ISSUES",
                "integration": "‚úÖ READY" if integration_status.get("overall_status", "").startswith("‚úÖ") else "‚ùå ISSUES",
                "workflow": "‚úÖ READY" if e2e_status.get("summary", {}).get("overall_status", "").startswith("‚úÖ") else "‚ùå ISSUES",
                "performance": "‚úÖ READY" if perf_status.get("summary", {}).get("status", "").startswith("‚úÖ") else "‚ö†Ô∏è CONCERNS"
            }
            
            # Integration Quality Assessment  
            passing_components = sum(1 for status in assessment["component_status"].values() if status.startswith("‚úÖ"))
            total_components = len(assessment["component_status"])
            
            assessment["integration_quality"] = {
                "passing_components": passing_components,
                "total_components": total_components,
                "pass_rate": passing_components / total_components * 100,
                "quality_score": "HIGH" if passing_components == total_components else 
                               "MEDIUM" if passing_components >= total_components * 0.75 else "LOW"
            }
            
            # Readiness Assessment
            if passing_components == total_components:
                assessment["overall_status"] = "‚úÖ READY_FOR_PHASE_1"
                assessment["readiness_assessment"] = {
                    "phase_0_5_complete": True,
                    "integration_validated": True,
                    "performance_acceptable": True,
                    "ready_for_scaling": True
                }
                assessment["recommendations"] = [
                    "Proceed to Phase 1 infrastructure development",
                    "Begin database integration planning",
                    "Design web interface architecture",
                    "Plan human evaluation framework"
                ]
            elif passing_components >= total_components * 0.75:
                assessment["overall_status"] = "‚ö†Ô∏è MOSTLY_READY"
                assessment["readiness_assessment"] = {
                    "phase_0_5_complete": True,
                    "integration_mostly_validated": True,
                    "minor_issues_present": True,
                    "ready_for_limited_scaling": True
                }
                assessment["recommendations"] = [
                    "Address minor integration issues before Phase 1",
                    "Consider limited Phase 1 pilot",
                    "Strengthen error handling in identified components",
                    "Plan incremental scaling approach"
                ]
            else:
                assessment["overall_status"] = "‚ùå NOT_READY"
                assessment["readiness_assessment"] = {
                    "phase_0_5_incomplete": True,
                    "significant_integration_issues": True,
                    "requires_remediation": True,
                    "not_ready_for_scaling": True
                }
                assessment["recommendations"] = [
                    "Do NOT proceed to Phase 1",
                    "Address critical integration failures",
                    "Redesign failing components",
                    "Re-run integration testing after fixes"
                ]
            
            print(f"‚úÖ Final assessment: {assessment['overall_status']}")
            
        except Exception as e:
            assessment["overall_status"] = "‚ùå ASSESSMENT_FAILED"
            assessment["error"] = str(e)
            print(f"‚ùå Assessment generation failed: {str(e)}")
        
        return assessment


def main():
    """Main execution function."""
    parser = argparse.ArgumentParser(description="Phase 0.5 Integration Testing")
    parser.add_argument("--mock", action="store_true", help="Run in mock mode (no API calls)")
    parser.add_argument("--verbose", action="store_true", help="Verbose output")
    parser.add_argument("--output", type=str, help="Output directory for results")
    
    args = parser.parse_args()
    
    # Initialize tester
    tester = Phase05IntegrationTester(mock_mode=args.mock)
    
    # Run comprehensive test
    results = tester.run_comprehensive_integration_test()
    
    # Output results
    if args.output:
        output_dir = Path(args.output)
        output_dir.mkdir(exist_ok=True)
        
        # Save detailed results
        with open(output_dir / f"phase_0_5_integration_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", "w") as f:
            json.dump(results, f, indent=2, default=str)
        
        # Save summary
        final_assessment = results.get("final_assessment", {})
        summary = {
            "status": final_assessment.get("overall_status", "UNKNOWN"),
            "component_status": final_assessment.get("component_status", {}),
            "recommendations": final_assessment.get("recommendations", [])
        }
        
        with open(output_dir / "integration_summary.json", "w") as f:
            json.dump(summary, f, indent=2, default=str)
        
        print(f"\nüìÅ Results saved to: {output_dir}")
    
    # Print summary
    print("\n" + "="*60)
    print("PHASE 0.5 INTEGRATION TEST SUMMARY")
    print("="*60)
    
    final_assessment = results.get("final_assessment", {})
    print(f"Overall Status: {final_assessment.get('overall_status', 'UNKNOWN')}")
    
    component_status = final_assessment.get("component_status", {})
    for component, status in component_status.items():
        print(f"{component.title()}: {status}")
    
    recommendations = final_assessment.get("recommendations", [])
    if recommendations:
        print("\nRecommendations:")
        for i, rec in enumerate(recommendations, 1):
            print(f"{i}. {rec}")
    
    # Exit code based on status
    if final_assessment.get("overall_status", "").startswith("‚úÖ"):
        sys.exit(0)
    elif final_assessment.get("overall_status", "").startswith("‚ö†Ô∏è"):
        sys.exit(1)
    else:
        sys.exit(2)


if __name__ == "__main__":
    main()